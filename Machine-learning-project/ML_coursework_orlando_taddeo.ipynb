{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37d33ac",
   "metadata": {},
   "source": [
    "**Please note that the WAME implementation used is compatible with tensorflow 2.0.0 only.\n",
    "Please run this notebook in an environment where tensorflow 2.0.0 is installed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98044fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is just to check what version of tensorflow\n",
    "#is installed on your system\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e819425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d72d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define here the WAMEprop class, to be used later\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "\n",
    "\n",
    "class WAMEprop(optimizer_v2.OptimizerV2):\n",
    "\t\"\"\"WAME optimizer.\n",
    "\tIt is recommended to leave the parameters of this optimizer at their default values as these have been\n",
    "\tshown empircally to deliver good results (except the learning rate, which can be freely tuned).\n",
    "\tThe algorithm has been adapated slightly from the original paper to replace \\frac{1}{\\theta} with \\frac{1}{\\sqrt{\\theta}} after \n",
    "\tspeaking with the algorithm developers. \n",
    "\t# Arguments\n",
    "\t\tlearning_rate: float >= 0. Base learning rate.\n",
    "\t\talpha: float >= 0. Decay rate of the exponentially weighted moving average.\n",
    "\t\teta_plus: float > 0. Multiplicative term of the acceleration factor for the case of a positive gradient product.\n",
    "\t\teta_minus: float > 0. Multiplicative term of the acceleration factor for the case of a negative gradient product.\n",
    "\t\tzeta_min: float > 0. Lower bounding value for the accerlation factor.\n",
    "\t\tzeta_max: float > 0. Upper bounding value for the acceleration factor.\n",
    "\t\tepsilon: float > 0. A very small fudge factor requried to avoid a possible division by zero error.\n",
    "\t# References\n",
    "\t\t- [wame: Training Convolutional Networks with Weightâ€“wise Adaptive Learning Rates]\n",
    "\t\t(https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2017-50.pdf)\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, learning_rate=0.0001, alpha = 0.9, eta_plus = 1.2, eta_minus = 0.1, zeta_min = 0.01, zeta_max = 100, epsilon = 1e-11, **kwargs):\n",
    "\t\tsuper(WAMEprop, self).__init__(**kwargs)\n",
    "\t\tself._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "\t\tself._set_hyper(\"alpha\", alpha)\n",
    "\t\tself._set_hyper(\"eta_plus\", eta_plus)\n",
    "\t\tself._set_hyper(\"eta_minus\", eta_minus)\n",
    "\t\tself._set_hyper(\"zeta_min\", zeta_min)\n",
    "\t\tself._set_hyper(\"zeta_max\", zeta_max)\n",
    "\t\tself.epsilon = epsilon\n",
    " \n",
    "\tdef _create_slots(self, var_list):\n",
    "\t\tfor var in var_list:\n",
    "\t\t\tself.add_slot(var, \"zetas\")\n",
    "\t\t\tself.add_slot(var, \"zeds\")\n",
    "\t\t\tself.add_slot(var, \"thetas\")\n",
    "\t\t\tself.add_slot(var, \"old_grads\")\n",
    "\n",
    "\tdef _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "\t\tsuper(WAMEprop, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "\t\talpha = array_ops.identity(self._get_hyper(\"alpha\", var_dtype))\n",
    "\t\teta_plus = array_ops.identity(self._get_hyper(\"eta_plus\", var_dtype))\n",
    "\t\teta_minus = array_ops.identity(self._get_hyper(\"eta_minus\", var_dtype))\n",
    "\t\tzeta_max = array_ops.identity(self._get_hyper(\"zeta_max\", var_dtype))\n",
    "\t\tzeta_min = array_ops.identity(self._get_hyper(\"zeta_min\", var_dtype))\n",
    "\t\tapply_state[(var_device, var_dtype)].update(\n",
    "\t\t\tdict(\n",
    "\t\t\t\tepsilon=ops.convert_to_tensor_v2(self.epsilon, var_dtype),\n",
    "\t\t\t\talpha=alpha,\n",
    "\t\t\t\teta_plus = eta_plus,\n",
    "\t\t\t\teta_minus = eta_minus,\n",
    "\t\t\t\tzeta_max = zeta_max,\n",
    "\t\t\t\tzeta_min = zeta_min,\n",
    "\t\t\t\tone_minus_alpha = 1 - alpha))\n",
    "\n",
    "\tdef _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "\t\tvar_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "\t\tcoefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "\t\t\t\t\t\tor self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "\t\tzeta =  self.get_slot(var, 'zetas')\n",
    "\t\tzed =  self.get_slot(var, 'zeds')\n",
    "\t\ttheta =  self.get_slot(var, 'thetas')\n",
    "\t\told_grad =  self.get_slot(var, 'old_grads')\n",
    "\n",
    "\t\tnew_z = tf.where(\n",
    "\t\t\tmath_ops.Equal(x = grad * old_grad, y = 0),\n",
    "\t\tzeta,\n",
    "\t\ttf.where(math_ops.Greater(x = grad * old_grad, y = 0),\n",
    "\t\t\tx = math_ops.Minimum(x = zeta * coefficients['eta_plus'], y = coefficients['zeta_max']),\n",
    "\t\t\ty = math_ops.Maximum(x = zeta * coefficients['eta_minus'], y = coefficients['zeta_min'])\n",
    "\t\t)\n",
    "\t\t)\n",
    "\t\t\t\n",
    "\t\tnew_z = state_ops.assign(zeta, new_z, use_locking=self._use_locking)\n",
    "\n",
    "\t\tnew_zed = (coefficients[\"alpha\"] * zed)  + (coefficients[\"one_minus_alpha\"]*new_z)\n",
    "\t\tnew_zed = state_ops.assign(zed, new_zed, use_locking=self._use_locking)\n",
    "\n",
    "\t\tnew_t = (coefficients[\"alpha\"] * theta)  + (coefficients[\"one_minus_alpha\"]*math_ops.square(grad))\n",
    "\t\tnew_t = state_ops.assign(theta, new_t, use_locking=self._use_locking)\n",
    "\n",
    "\t\tvar_t  = var - (coefficients[\"lr_t\"] * new_zed * grad * (1/(math_ops.Sqrt(x = new_t) +coefficients[\"epsilon\"])))\n",
    "\n",
    "\t\told_grad = state_ops.assign(old_grad, grad, use_locking=self._use_locking)\n",
    "\n",
    "\t\treturn state_ops.assign(var, var_t, use_locking=self._use_locking).op\n",
    "\n",
    "\tdef _resource_apply_sparse(self, grad, var):\n",
    "\t\traise NotImplementedError(\"Sparse gradient updates are not supported.\")\n",
    "\t\n",
    "\tdef get_config(self):\n",
    "\t\tconfig = super(WAMEprop, self).get_config()\n",
    "\t\tconfig.update({'learning_rate': self._serialize_hyperparameter(\"learning_rate\"),\n",
    "\t\t\t\t  'alpha': self._serialize_hyperparameter(\"alpha\"),\n",
    "\t\t\t\t  'eta_plus': self._serialize_hyperparameter(\"eta_plus\"),\n",
    "\t\t\t\t  'eta_minus': self._serialize_hyperparameter(\"eta_minus\"),\n",
    "\t\t\t\t  'zeta_min': self._serialize_hyperparameter(\"zeta_min\"),\n",
    "\t\t\t\t  'zeta_max': self._serialize_hyperparameter(\"zeta_max\") \n",
    "\t\t\t\t  })\n",
    "\t\treturn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the functions to build and fit a model, to be used later\n",
    "\n",
    "def build_model(train_data, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_units, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(Dense(n_units, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def fit_model(model, train_features, train_targets, test_features, test_targets, num_epochs):\n",
    "    history = model.fit(train_features.to_numpy(), train_targets.to_numpy(), validation_data=(test_features.to_numpy(), test_targets.to_numpy()),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    return history\n",
    "\n",
    "def build_model_wame(train_data, n_units, lambda_rate=0.0001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_units, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(Dense(n_units, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=WAMEprop(name='wame', lr=lambda_rate), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_white = input('''\n",
    "                   Please paste here the complete path\n",
    "                   to the white wine csv file:\n",
    "                   ''' \n",
    "                   )\n",
    "white_df = pd.read_csv(path_white, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b70533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check on the dataframe shape\n",
    "\n",
    "white_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb4094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glimpse at the data\n",
    "\n",
    "white_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We separate the features form the target value,\n",
    "#to pass them the Keras model.fit() method \n",
    "\n",
    "white_features_df = white_df.drop('quality', axis=1)\n",
    "white_targets = white_df['quality'] #Note this is a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e667988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code executes the K-Fold cross validation to establish a sensible\n",
    "#number of epochs to avoid overfitting\n",
    "\n",
    "k_fold_validator = KFold(n_splits=10)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "units_number = 32\n",
    "epochs_number = 100\n",
    "\n",
    "all_folds_mae_history = []\n",
    "#this list will contain other lists, each of which will contain all the mae value for each epoch, for a certain fold\n",
    "\n",
    "count = 0\n",
    "\n",
    "for train_indices, test_indices in k_fold_validator.split(white_features_df):\n",
    "    \n",
    "  white_train_features_df = white_features_df.iloc[train_indices, :]\n",
    "  white_train_features_df_s = pd.DataFrame(scaler.fit_transform(white_train_features_df),\n",
    "                                             columns=white_train_features_df.columns)\n",
    "\n",
    "  white_train_targets = white_targets[train_indices]\n",
    "    \n",
    "  white_test_features_df = white_features_df.iloc[test_indices, :]\n",
    "  white_test_features_df_s = pd.DataFrame(scaler.fit_transform(white_test_features_df),\n",
    "                                             columns=white_test_features_df.columns)\n",
    "\n",
    "  white_test_targets = white_targets[test_indices]\n",
    "\n",
    "  model = build_model(white_train_features_df_s, units_number)\n",
    "  history = fit_model(model, white_train_features_df_s, white_train_targets, white_test_features_df_s, white_test_targets, epochs_number)\n",
    "  history_dict = history.history\n",
    "  fold_mae_history = history_dict['val_mae'] # this list contains the value of mae for each epoch, for this fold\n",
    "  all_folds_mae_history.append(fold_mae_history)\n",
    "  count += 1\n",
    "  print('Fold ' + str(count) + ' done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the k-fold cross-validation values of MAE for each epoch\n",
    "#we need to average those for each fold\n",
    "\n",
    "average_mae_history = [np.mean([x[j] for x in all_folds_mae_history]) \n",
    "                       for j in range(epochs_number)]\n",
    "\n",
    "#This code plots the result of the 10-fold cross validation\n",
    "#with 100 epochs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(epochs_number), average_mae_history)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Mean absolute error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run once again the cross-validation procedure,\n",
    "#but limiting the number of epochs to 20, given the results above\n",
    "\n",
    "k_fold_validator = KFold(n_splits=10)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "units_number = 32\n",
    "epochs_number = 20\n",
    "\n",
    "all_folds_mae_history = []\n",
    "#this list will contain other lists, each of which will contain all the mae value for each epoch, for a certain fold\n",
    "\n",
    "count = 0\n",
    "\n",
    "for train_indices, test_indices in k_fold_validator.split(white_features_df):\n",
    "    \n",
    "  white_train_features_df = white_features_df.iloc[train_indices, :]\n",
    "  white_train_features_df_s = pd.DataFrame(scaler.fit_transform(white_train_features_df),\n",
    "                                             columns=white_train_features_df.columns)\n",
    "\n",
    "  white_train_targets = white_targets[train_indices]\n",
    "    \n",
    "  white_test_features_df = white_features_df.iloc[test_indices, :]\n",
    "  white_test_features_df_s = pd.DataFrame(scaler.fit_transform(white_test_features_df),\n",
    "                                             columns=white_test_features_df.columns)\n",
    "\n",
    "  white_test_targets = white_targets[test_indices]\n",
    "\n",
    "  model = build_model(white_train_features_df_s, units_number)\n",
    "  history = fit_model(model, white_train_features_df_s, white_train_targets, white_test_features_df_s, white_test_targets, epochs_number)\n",
    "  history_dict = history.history\n",
    "  fold_mae_history = history_dict['val_mae'] # this list contains the value of mae for each epoch, for this fold\n",
    "  all_folds_mae_history.append(fold_mae_history)\n",
    "  count += 1\n",
    "  print('Fold ' + str(count) + ' done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb504a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we plot the results once again, but for 20 epochs\n",
    "\n",
    "average_mae_history = [np.mean([x[j] for x in all_folds_mae_history]) \n",
    "                       for j in range(epochs_number)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(epochs_number), average_mae_history)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Mean absolute error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3011db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now follow the same procedure as before, using the WAMEprop optimiser,\n",
    "#to find the optimal number of epochs\n",
    "\n",
    "k_fold_validator = KFold(n_splits=10)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "units_number = 32\n",
    "epochs_number = 200\n",
    "\n",
    "all_folds_mae_history = []\n",
    "#this list will contain other lists, each of which will contain all the mae value for each epoch, for a certain fold\n",
    "\n",
    "count = 0\n",
    "\n",
    "for train_indices, test_indices in k_fold_validator.split(white_features_df):\n",
    "    \n",
    "  white_train_features_df = white_features_df.iloc[train_indices, :]\n",
    "  white_train_features_df_s = pd.DataFrame(scaler.fit_transform(white_train_features_df),\n",
    "                                             columns=white_train_features_df.columns)\n",
    "\n",
    "  white_train_targets = white_targets[train_indices]\n",
    "    \n",
    "  white_test_features_df = white_features_df.iloc[test_indices, :]\n",
    "  white_test_features_df_s = pd.DataFrame(scaler.fit_transform(white_test_features_df),\n",
    "                                             columns=white_test_features_df.columns)\n",
    "\n",
    "  white_test_targets = white_targets[test_indices]\n",
    "\n",
    "  model = build_model_wame(white_train_features_df_s, units_number)\n",
    "  history = fit_model(model, white_train_features_df_s, white_train_targets, white_test_features_df_s, white_test_targets, epochs_number)\n",
    "  history_dict = history.history\n",
    "  fold_mae_history = history_dict['val_mae'] # this list contains the value of mae for each epoch, for this fold\n",
    "  all_folds_mae_history.append(fold_mae_history)\n",
    "  count += 1\n",
    "  print('Fold ' + str(count) + ' done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot the results once again\n",
    "\n",
    "average_mae_history = [np.mean([x[j] for x in all_folds_mae_history]) \n",
    "                       for j in range(epochs_number)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(epochs_number), average_mae_history)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Mean absolute error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now run the sensitivity analysis for the WAMEprop optimiser\n",
    "\n",
    "learning_rates = [0.0001, 0.0002, 0.0005, 0.001]\n",
    "units_number = 32\n",
    "epochs_number = 75 #we will use 75 for the real test\n",
    "average_mae_history_all_lambdas = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "\n",
    "  k_fold_validator = KFold(n_splits=10)\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  all_folds_mae_history = []\n",
    "  #this list will contain other lists, each of which will contain all the mae value for each epoch, for a certain fold\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  for train_indices, test_indices in k_fold_validator.split(white_features_df):\n",
    "      \n",
    "    white_train_features_df = white_features_df.iloc[train_indices, :]\n",
    "    white_train_features_df_s = pd.DataFrame(scaler.fit_transform(white_train_features_df),\n",
    "                                              columns=white_train_features_df.columns)\n",
    "\n",
    "    white_train_targets = white_targets[train_indices]\n",
    "      \n",
    "    white_test_features_df = white_features_df.iloc[test_indices, :]\n",
    "    white_test_features_df_s = pd.DataFrame(scaler.fit_transform(white_test_features_df),\n",
    "                                              columns=white_test_features_df.columns)\n",
    "\n",
    "    white_test_targets = white_targets[test_indices]\n",
    "\n",
    "    model = build_model_wame(white_train_features_df_s, units_number, lambda_rate=learning_rate)\n",
    "    history = fit_model(model, white_train_features_df_s, white_train_targets, white_test_features_df_s, white_test_targets, epochs_number)\n",
    "    history_dict = history.history\n",
    "    fold_mae_history = history_dict['val_mae'] # this list contains the value of mae for each epoch, for this fold\n",
    "    all_folds_mae_history.append(fold_mae_history)\n",
    "    count += 1\n",
    "    print('Fold ' + str(count) + ' done.')\n",
    "  \n",
    "  average_mae_history = [np.mean([x[j] for x in all_folds_mae_history]) for j in range(epochs_number)]\n",
    "  average_mae_history_all_lambdas.append(average_mae_history)\n",
    "\n",
    "  print('Learning rate = ' + str(learning_rate) + ' done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now plot the sensitivity analysis results\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(len(average_mae_history_all_lambdas)):\n",
    "  ax.plot(range(epochs_number), average_mae_history_all_lambdas[i])\n",
    "  ax.set_xlabel('Epochs')\n",
    "  ax.set_ylabel('Mean absolute error')\n",
    "ax.legend(['lr=0.0001', 'lr=0.0002', 'lr=0.0005', 'lr=0.001'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
